{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9138b87-7413-4bcb-8005-d6a019bf2c6d",
   "metadata": {},
   "source": [
    "# <center>Genre Classification Using Extracted Audio Features</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97226ad-8622-4b76-8f1f-c2a0fa0c30c9",
   "metadata": {},
   "source": [
    "<center> Levi Davis <br> ljd3frf@virginia.edu <br> CS 6501: Digital Signal Processing, Spring 2023 </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df14092e-59fc-42dc-be35-fb162af44466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "from PIL import Image\n",
    "import scipy as stats \n",
    "import warnings\n",
    "from pretty_html_table import build_table\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42452ad3-7d1d-4dd8-9765-799b6bb60453",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da736298-41a5-4c0b-b3ec-c8366cd2e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data folder\n",
    "# Puts all songs into one folder\n",
    "\n",
    "# Define the source directory where the subfolders are located\n",
    "source_dir = os.getcwd() + '/data/fma_small/fma_small'\n",
    "\n",
    "# Define the destination directory where you want the files to be moved\n",
    "destination_dir = os.getcwd() +'/data/data_small'\n",
    "\n",
    "# Loop through all the subdirectories in the source directory\n",
    "for subdir in os.listdir(source_dir):\n",
    "    # Define the full path to the subdirectory\n",
    "    subdir_path = os.path.join(source_dir, subdir)\n",
    "    # Check if the item in the directory is a directory\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Loop through all the files in the subdirectory\n",
    "        for file in os.listdir(subdir_path):\n",
    "            # Define the full path to the file\n",
    "            file_path = os.path.join(subdir_path, file)\n",
    "            \n",
    "            # Move the file to the destination directory\n",
    "            shutil.move(file_path, destination_dir)\n",
    "        \n",
    "        # Remove the empty subdirectory\n",
    "        os.rmdir(subdir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc162acf-65e8-41e1-80a9-059c445e77ef",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0e368c-f035-49af-bd80-cf8539178cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeled from https://github.com/mdeff/fma/blob/master/features.py\n",
    "\n",
    "def columns():\n",
    "    feature_sizes = dict(chroma_stft=12, chroma_cqt=12, chroma_cens=12,\n",
    "                         tonnetz=6, mfcc=20, rms=1, zcr=1,\n",
    "                         spectral_centroid=1, spectral_bandwidth=1,\n",
    "                         spectral_contrast=7, spectral_rolloff=1, tempogram_ratio=13)\n",
    "    moments = ('mean', 'std', 'skew', 'kurtosis', 'median', 'min', 'max')\n",
    "\n",
    "    columns = []\n",
    "    for name, size in feature_sizes.items():\n",
    "        for moment in moments:\n",
    "            it = ((name, moment, '{:02d}'.format(i+1)) for i in range(size))\n",
    "            columns.extend(it)\n",
    "\n",
    "    names = ('feature', 'statistics', 'number')\n",
    "    columns = pd.MultiIndex.from_tuples(columns, names=names)\n",
    "\n",
    "    # More efficient to slice if indexes are sorted.\n",
    "    return columns.sort_values()\n",
    "\n",
    "\n",
    "def compute_features(tid, filepath):\n",
    "\n",
    "    features = pd.Series(index=columns(), dtype=np.float32, name=tid)\n",
    "\n",
    "\n",
    "    def feature_stats(name, values):\n",
    "        features[name, 'mean'] = np.mean(values, axis=1)\n",
    "        features[name, 'std'] = np.std(values, axis=1)\n",
    "        features[name, 'skew'] = stats.stats.skew(values, axis=1)\n",
    "        features[name, 'kurtosis'] = stats.stats.kurtosis(values, axis=1)\n",
    "        features[name, 'median'] = np.median(values, axis=1)\n",
    "        features[name, 'min'] = np.min(values, axis=1)\n",
    "        features[name, 'max'] = np.max(values, axis=1)\n",
    "\n",
    "    try:\n",
    "        x, sr = librosa.load(filepath, sr=None, mono=True)  # kaiser_fast\n",
    "\n",
    "        f = librosa.feature.zero_crossing_rate(x, frame_length=2048, hop_length=512)\n",
    "        feature_stats('zcr', f)\n",
    "        \n",
    "        f = librosa.feature.tempogram_ratio(y=x)\n",
    "        feature_stats('tempogram_ratio', f)\n",
    "        \n",
    "        cqt = np.abs(librosa.cqt(x, sr=sr, hop_length=512, bins_per_octave=12,\n",
    "                                 n_bins=7*12, tuning=None))\n",
    "        assert cqt.shape[0] == 7 * 12\n",
    "        assert np.ceil(len(x)/512) <= cqt.shape[1] <= np.ceil(len(x)/512)+1\n",
    "\n",
    "        f = librosa.feature.chroma_cqt(C=cqt, n_chroma=12, n_octaves=7)\n",
    "        feature_stats('chroma_cqt', f)\n",
    "        f = librosa.feature.chroma_cens(C=cqt, n_chroma=12, n_octaves=7)\n",
    "        feature_stats('chroma_cens', f)\n",
    "        f = librosa.feature.tonnetz(chroma=f)\n",
    "        feature_stats('tonnetz', f)\n",
    "\n",
    "        del cqt\n",
    "        stft = np.abs(librosa.stft(x, n_fft=2048, hop_length=512))\n",
    "        assert stft.shape[0] == 1 + 2048 // 2\n",
    "        assert np.ceil(len(x)/512) <= stft.shape[1] <= np.ceil(len(x)/512)+1\n",
    "        del x\n",
    "\n",
    "        f = librosa.feature.chroma_stft(S=stft**2, n_chroma=12)\n",
    "        feature_stats('chroma_stft', f)\n",
    "\n",
    "        f = librosa.feature.rms(S=stft)\n",
    "        feature_stats('rms', f)\n",
    "        \n",
    "\n",
    "        f = librosa.feature.spectral_centroid(S=stft)\n",
    "        feature_stats('spectral_centroid', f)\n",
    "        f = librosa.feature.spectral_bandwidth(S=stft)\n",
    "        feature_stats('spectral_bandwidth', f)\n",
    "        f = librosa.feature.spectral_contrast(S=stft, n_bands=6)\n",
    "        feature_stats('spectral_contrast', f)\n",
    "        f = librosa.feature.spectral_rolloff(S=stft)\n",
    "        feature_stats('spectral_rolloff', f)\n",
    "        \n",
    "        mel = librosa.feature.melspectrogram(sr=sr, S=stft**2)\n",
    "        del stft\n",
    "        f = librosa.feature.mfcc(S=librosa.power_to_db(mel), n_mfcc=20)\n",
    "        feature_stats('mfcc', f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('{}: {}'.format(tid, repr(e)))\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5c628-4808-4b46-bf07-2e314ad4a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where mp3 are stored.\n",
    "DATA_DIR = '\\\\data\\\\data_small\\\\'\n",
    "DATA_PATH = os.getcwd() + DATA_DIR\n",
    "song_files = os.listdir(DATA_PATH)\n",
    "\n",
    "# Loop through all song files, compute all feature info, and store df as csv\n",
    "series_list = []\n",
    "for file in song_files:\n",
    "    track_id = file[-10:-4]\n",
    "    file_path = os.path.join(DATA_PATH, file)\n",
    "    feature_series = compute_features(track_id, file_path)\n",
    "    series_list.append(feature_series)\n",
    "    \n",
    "DF = pd.DataFrame(series_list)\n",
    "\n",
    "# Save to csv\n",
    "DF.to_csv('computed_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d966ac9-a085-4efd-8a8a-9725e88db3c3",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2caf8-aacc-4706-bb88-8392012f56a4",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0aeee38-a036-4660-b05e-01c6673986dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7951, 609)\n"
     ]
    }
   ],
   "source": [
    "# Load data from csv\n",
    "SONG_DATA = pd.read_csv('computed_features.csv',header=[0,1,2], index_col= 0)\n",
    "SONG_DATA.index.name = 'track_id'\n",
    "# Remove songs with NAN values\n",
    "SONG_DATA.dropna(inplace=True)\n",
    "print(SONG_DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd05015c-2d4a-4802-bb22-d8dc042e7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_info_dir = os.getcwd() + '/data/fma_metadata/tracks.csv'\n",
    "tracks = pd.read_csv(track_info_dir, index_col=0, header=[0,1])\n",
    "\n",
    "# using small dataset of 8,000 songs\n",
    "track_small = tracks[tracks['set', 'subset'] == 'small']\n",
    "genres = track_small['track','genre_top']\n",
    "genres.name = 'genre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5b4a24-7cfd-43a6-b273-037d20bbbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure features and labels obs are equal\n",
    "# get the indices that are not present in both the dataframe and the series\n",
    "idx_to_drop = SONG_DATA.index.symmetric_difference(genres.index)\n",
    "# drop the rows from the dataframe\n",
    "try:\n",
    "    SONG_DATA.drop(idx_to_drop, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "# drop the rows from the labels\n",
    "try:\n",
    "    genres.drop(idx_to_drop, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "assert genres.shape[0] == SONG_DATA.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c08cd-8dbe-4397-873d-15b13b1cb75e",
   "metadata": {},
   "source": [
    "### Models with one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0087183-0e9b-4c1b-8d95-4523d5d7aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = pd.DataFrame(columns= ['classifier', 'feature', 'test_accuracy', 'duration'])\n",
    "\n",
    "classifiers = [SVC(), RandomForestClassifier(), KNeighborsClassifier(), \n",
    "               DecisionTreeClassifier(), GradientBoostingClassifier()]\n",
    "classifier_names = ['SVC', 'RandomForest', 'KNN', 'DecisionTree', 'GradientBoosting']\n",
    "\n",
    "features = SONG_DATA.columns.levels[0].values\n",
    "for feature in features:\n",
    "    # Separate the data into features and labels\n",
    "    X = SONG_DATA[feature]\n",
    "    y = genres\n",
    "    # Split the data into train, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, \n",
    "                                                        random_state=51, stratify=y)\n",
    "    \n",
    "    # Be sure training samples are shuffled.\n",
    "    X_train, y_train = skl.utils.shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "    # Standardize features by removing the mean and scaling to unit variance.\n",
    "    scaler = skl.preprocessing.StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    for i,cl in enumerate(classifiers):\n",
    "        # Support vector classification.\n",
    "        clf = cl\n",
    "        start = time.time() # start timer\n",
    "        clf.fit(X_train, y_train)\n",
    "        stop = time.time() # stop time\n",
    "        duration = stop-start # compute time to run algo\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Append the results to the DataFrame\n",
    "        RESULTS.loc[len(RESULTS)] = [classifier_names[i], feature, score, duration]\n",
    "\n",
    "\n",
    "RESULTS.to_csv('ML_single_feature_classification_results.csv'. index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7838a1a-a5e7-4d17-8efb-6938691e6741",
   "metadata": {},
   "source": [
    "### Models with two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f53634b7-ea9b-40c1-b9c6-78d85ecabc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PAIRS = pd.DataFrame(columns= ['classifier', 'feature_pair', 'test_accuracy','duration'])\n",
    "\n",
    "classifiers = [SVC(), RandomForestClassifier(), KNeighborsClassifier(), \n",
    "               DecisionTreeClassifier(), GradientBoostingClassifier()]\n",
    "classifier_names = ['SVC', 'RandomForest', 'KNN', 'DecisionTree', 'GradientBoosting']\n",
    "\n",
    "features = SONG_DATA.columns.levels[0].values\n",
    "pairs = [[a, b] for idx, a in enumerate(features) for b in features[idx + 1:]]\n",
    "\n",
    "for feature_pair in pairs:\n",
    "    # Separate the data into features and labels\n",
    "    X = SONG_DATA[feature_pair]\n",
    "    y = genres\n",
    "    # Split the data into train, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, \n",
    "                                                        random_state=51, stratify=y)\n",
    "    \n",
    "    # Be sure training samples are shuffled.\n",
    "    X_train, y_train = skl.utils.shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "    # Standardize features by removing the mean and scaling to unit variance.\n",
    "    scaler = skl.preprocessing.StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    for i,cl in enumerate(classifiers):\n",
    "        # Support vector classification.\n",
    "        clf = cl\n",
    "        start = time.time() # start timer\n",
    "        clf.fit(X_train, y_train)\n",
    "        stop = time.time() # stop time\n",
    "        duration = stop-start # compute time to run algo\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Append the results to the DataFrame\n",
    "        RESULTS_PAIRS.loc[len(RESULTS_PAIRS)] = [classifier_names[i], feature_pair, score, duration]\n",
    "\n",
    "\n",
    "RESULTS_PAIRS.to_csv('ML_two_feature_classification_results.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e0d24-fcff-40bf-93eb-65c5cfb49e4e",
   "metadata": {},
   "source": [
    "### Models with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0664b1f2-b4ce-4e9f-b2f2-60d30b6fd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_ALL = pd.DataFrame(columns= ['classifier', 'test_accuracy', 'duration'])\n",
    "\n",
    "classifiers = [SVC(), RandomForestClassifier(), KNeighborsClassifier(), \n",
    "               DecisionTreeClassifier(), GradientBoostingClassifier()]\n",
    "classifier_names = ['SVC', 'RandomForest', 'KNN', 'DecisionTree', 'GradientBoosting']\n",
    "\n",
    "\n",
    "# Separate the data into features and labels\n",
    "X = SONG_DATA\n",
    "y = genres\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=51, stratify=y)\n",
    "\n",
    "# Be sure training samples are shuffled.\n",
    "X_train, y_train = skl.utils.shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = skl.preprocessing.StandardScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "for i,cl in enumerate(classifiers):\n",
    "    # Support vector classification.\n",
    "    clf = cl\n",
    "    start = time.time() # start timer\n",
    "    clf.fit(X_train, y_train)\n",
    "    stop = time.time() # stop time\n",
    "    duration = stop-start # compute time to run algo\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    RESULTS_ALL.loc[len(RESULTS_ALL)] = [classifier_names[i], score, duration]\n",
    "    \n",
    "    \n",
    "RESULTS_ALL.to_csv('ML_all_features_classification_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0527c-7698-4a64-ac86-db34cc0ca4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf131ed-9b8e-430d-98ff-df29ee563ebf",
   "metadata": {},
   "source": [
    "# Second round: model hyperparamter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fab22d3-1cf9-4324-8453-41118a9df88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top two classifiers\n",
    "classifiers = [GradientBoostingClassifier(), RandomForestClassifier()]\n",
    "classifier_names = ['Gradient Boosting', 'Random Forest']\n",
    "# top 2 single features and top 4 feature pairs\n",
    "features = ['mfcc', 'spectral_contrast', ['mfcc', 'spectral_contrast'], ['mfcc', 'chroma_cqt'],\n",
    "           ['mfcc', 'zcr'], ['mfcc', 'tempogram_ratio']]\n",
    "\n",
    "RESULTS = pd.DataFrame(columns= ['classifier', 'feature_pair', 'test_accuracy','duration'])\n",
    "\n",
    "for feature_pair in features:\n",
    "    # Separate the data into features and labels\n",
    "    X = SONG_DATA[feature_pair]\n",
    "    y = genres\n",
    "    # Split the data into train, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, \n",
    "                                                        random_state=51, stratify=y)\n",
    "    \n",
    "    # Be sure training samples are shuffled.\n",
    "    X_train, y_train = skl.utils.shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "    # Standardize features by removing the mean and scaling to unit variance.\n",
    "    scaler = skl.preprocessing.StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    for i,cl in enumerate(classifiers):\n",
    "        clf = cl\n",
    "        # start timer\n",
    "        start = time.time() # start timer\n",
    "        # Perform 5-fold cross-validation on the training set\n",
    "        cv_scores = cross_val_score(gbc, X_train, y_train, cv=5)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        stop = time.time() # stop time\n",
    "        duration = stop-start # compute time to run algo\n",
    "        score = clf.score(X_test, y_test)\n",
    "        # Append the results to the DataFrame\n",
    "        RESULTS.loc[len(RESULTS)] = [classifier_names[i], feature_pair, score, duration]\n",
    "\n",
    "\n",
    "RESULTS.to_csv('HPCV_classification_results.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae975ee8-0977-4e80-bcf1-f60797108099",
   "metadata": {},
   "source": [
    "# Make tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7193ec-84e9-4fb0-bebe-3fede2428ccd",
   "metadata": {},
   "source": [
    "### Single features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f880fa92-f792-407f-95d0-22bd192ab283",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pd.read_csv('ML_single_feature_classification_results.csv',index_col=0)\\\n",
    "            .sort_values('test_accuracy', ascending = False)\\\n",
    "            .reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c712674f-5df1-484c-9ef7-37b0ead89091",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_table = build_table(sf, 'blue_light', font_size='18px', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e0105f0e-a018-420a-8de9-648295be4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table\n",
    "with open('Images/single_feature_table.html', 'w') as f:\n",
    "    f.write(single_feature_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a636e967-4c79-4e4f-a234-5147d2e7b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_table_head = build_table(sf.head(10), 'blue_light', font_size='18px', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3214ee7b-4140-4182-acd5-b4c4ef16e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table\n",
    "with open('Images/single_feature_table_head.html', 'w') as f:\n",
    "    f.write(single_feature_table_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c95e7ac2-5f6d-48fd-91d5-a70f4524c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljd3frf\\AppData\\Local\\Temp\\ipykernel_8272\\409683886.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  sf_features = sf.groupby('classifier').mean().sort_values('test_accuracy',ascending=False).reset_index()\n"
     ]
    }
   ],
   "source": [
    "sf_features = sf.groupby('feature').mean().sort_values('test_accuracy',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bdd8bd48-8316-45eb-bf36-b56970ab6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_table_features = build_table(sf_features, 'blue_light', font_size='18px', index=True)\n",
    "# save table\n",
    "with open('Images/single_feature_table_features.html', 'w') as f:\n",
    "    f.write(single_feature_table_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991eb9e-1888-463d-89ff-0aa298661e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_algos = sf.groupby('classifier').mean().sort_values('test_accuracy',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "225dbd3e-81a8-4b24-ab6c-a2aec1ffd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature_table_algos = build_table(sf_algos, 'blue_light', font_size='18px', index=True)\n",
    "# save table\n",
    "with open('Images/single_feature_table_algos.html', 'w') as f:\n",
    "    f.write(single_feature_table_algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ca274-08cc-4792-b31a-b04640fc232e",
   "metadata": {},
   "source": [
    "### Two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87ccd27a-bf96-489f-86ab-0d65c8fb9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ML_two_feature_classification_results.csv')\\\n",
    "            .sort_values('test_accuracy', ascending = False)\\\n",
    "            .reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "55cc8cd4-9131-4728-bc3e-ffb8d84ef409",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_feature_table = build_table(df, 'blue_light', font_size='18px', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f2b639fc-defc-4c1d-aead-7b50e2b292c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table\n",
    "with open('Images/two_feature_table.html', 'w') as f:\n",
    "    f.write(two_feature_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fcd9c-5390-4f31-ad31-39763b4b9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_feature_table_head = build_table(df.head(10), 'blue_light', font_size='18px', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8207d6a1-2fc4-47b0-b6ed-d3cc99b3ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table\n",
    "with open('Images/two_feature_table_head.html', 'w') as f:\n",
    "    f.write(two_feature_table_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dc2a6b1b-2c36-4e6c-8f8e-fb6b10ebfe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljd3frf\\AppData\\Local\\Temp\\ipykernel_8272\\108149541.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_features = df.groupby('feature_pair').mean().sort_values('test_accuracy',ascending=False).reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_features = df.groupby('feature_pair').mean().sort_values('test_accuracy',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b3cddd1c-36cd-443a-bd4f-f4a35878f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_feature_table_features = build_table(df_features.head(10), 'blue_light', font_size='18px', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ab30164c-7625-45c0-a3bd-651841d4807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table\n",
    "with open('Images/two_feature_table_features.html', 'w') as f:\n",
    "    f.write(two_feature_table_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "aee3f3f4-e5ce-46ba-b897-85760936b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljd3frf\\AppData\\Local\\Temp\\ipykernel_8272\\2726571204.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_algos = df.groupby('classifier').mean().sort_values('test_accuracy',ascending=False).reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_algos = df.groupby('classifier').mean().sort_values('test_accuracy',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f3e15017-81a9-4eb3-b24e-ae545f3e14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_feature_table_algos = build_table(two_feature_table_algos.head(10), 'blue_light', font_size='18px', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b3e0fcc1-872a-4c1d-9263-278009cbdf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table\n",
    "with open('Images/two_feature_table_algos.html', 'w') as f:\n",
    "    f.write(two_feature_table_algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1734b0-c268-464e-9d88-c42af8b44909",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "76bb53b5-fab2-4066-a27d-d66a8330d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "af = pd.read_csv('ML_all_features_classification_results.csv')\\\n",
    "            .sort_values('test_accuracy', ascending = False)\\\n",
    "            .reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6b703ca6-17fe-44e2-b816-18e9a1d55c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_table = build_table(af, 'blue_light', font_size='18px', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "09a7b379-36f4-4393-b9f2-2f20a30d6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table\n",
    "with open('Images/all_feature_table.html', 'w') as f:\n",
    "    f.write(all_feature_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b68f7a-d2e3-456c-baae-cf36296a0b7f",
   "metadata": {},
   "source": [
    "### Tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f43a6521-dda7-416a-9c57-96da4a1bad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = pd.read_csv('HPCV_classification_results.csv',index_col=0)\\\n",
    "            .sort_values('test_accuracy', ascending = False)\\\n",
    "            .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4866f3b7-65a1-4b82-984d-9978ae121ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm__table = build_table(tm, 'blue_light', font_size='18px', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15735be3-839f-4ec3-a59d-7a8203b15d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table\n",
    "with open('Images/tm__table.html', 'w') as f:\n",
    "    f.write(tm__table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e30f1b-16ab-48cd-8eae-1c6cf1d40964",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "580dce15-a9c5-4553-a956-e65e0f098e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils, initializers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debed4db-a2bd-481f-9dc6-c8544e57a862",
   "metadata": {},
   "source": [
    "## Make Spectrogram for each audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf574b-4ccc-4977-a8c6-83d079df90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_metadata = \"data/fma_metadata/tracks.csv\"\n",
    "tracks = pd.read_csv(filename_metadata, header=2, low_memory=False)\n",
    "tracks_array = tracks.values\n",
    "tracks_id_array = tracks_array[: , 0]\n",
    "tracks_genre_array = tracks_array[: , 40]\n",
    "tracks_id_array = tracks_id_array.reshape(tracks_id_array.shape[0], 1)\n",
    "tracks_genre_array = tracks_genre_array.reshape(tracks_genre_array.shape[0], 1)\n",
    "\n",
    "folder_sample = \"data/fma_small\"\n",
    "directories = [d for d in os.listdir(folder_sample)\n",
    "               if os.path.isdir(os.path.join(folder_sample, d))]\n",
    "\n",
    "counter = 0\n",
    "if not os.path.exists('data/Train_Spectogram_Images_big'):\n",
    "    os.makedirs('data/Train_Spectogram_Images_big')\n",
    "for d in directories:\n",
    "    label_directory = os.path.join(folder_sample, d)\n",
    "    file_names = [os.path.join(label_directory, f)\n",
    "                  for f in os.listdir(label_directory)\n",
    "                  if f.endswith(\".mp3\")]\n",
    "    # Convert .mp3 files into mel-Spectograms\n",
    "    for f in file_names:\n",
    "        try:\n",
    "            track_id = f[-10:-4]\n",
    "            tracks_id_array1 = [str(s[0]).zfill(6) for s in tracks_id_array]\n",
    "            track_index = list(tracks_id_array1).index(str(track_id))\n",
    "            y, sr = librosa.load(f)\n",
    "            melspectrogram_array = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000)\n",
    "            mel = librosa.power_to_db(melspectrogram_array)\n",
    "            # Length and Width of Spectogram\n",
    "            fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "            fig_size[0] = float(mel.shape[1]) / float(50)\n",
    "            fig_size[1] = float(mel.shape[0]) / float(50)\n",
    "            plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "            plt.axis('off')\n",
    "            plt.axes([0., 0., 1., 1.0], frameon=False, xticks=[], yticks=[])\n",
    "            librosa.display.specshow(mel, cmap='gray_r')\n",
    "            plt.savefig(\"data/Train_Spectogram_Images_big/\"+str(track_id)+\"_\"+str(tracks_genre_array[track_index,0])+\".jpg\", bbox_inches=None, pad_inches=0)\n",
    "            plt.close()\n",
    "            counter = counter + 1\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebbc59-e0bc-4c1a-9ec6-adf687ac3f58",
   "metadata": {},
   "source": [
    "## Slice spectrograms into x-sec chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4c50ba7-c9f6-444f-91be-62b767895d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change frame_length below to change the length of slices\n",
    "def slice_mel_spectrogram(mel_spectrogram):\n",
    "    mel_spectrogram = mel_spectrogram[:,:]  # extract only one channel (e.g., the first channel)\n",
    "    frame_length = 256\n",
    "    spectrogram_length = mel_spectrogram.shape[1]\n",
    "    num_frames = int(np.floor(spectrogram_length / frame_length))\n",
    "    frames = np.zeros((num_frames, mel_spectrogram.shape[0], frame_length))\n",
    "    for i in range(num_frames):\n",
    "        start = i * frame_length\n",
    "        end = min(start + frame_length, spectrogram_length)\n",
    "        frames[i,:,:end-start] = mel_spectrogram[:,start:end]\n",
    "    return frames\n",
    "\n",
    "image_folder = \"data/Train_Spectogram_Images_big\"\n",
    "filenames = [os.path.join(image_folder, f) for f in os.listdir(image_folder)\n",
    "               if f.endswith(\".jpg\")]\n",
    "if not os.path.exists('data/Train_Sliced_Images_big'):\n",
    "    os.makedirs('data/Train_Sliced_Images_big')\n",
    "for f in filenames:\n",
    "    track_id = f.split('\\\\')[-1].split('_')[0]\n",
    "    genre_variable = re.search(r\"_([^_]+)\\.jpg$\", f).group(1)\n",
    "    img = Image.open(f).convert('L')\n",
    "    mel_spectrogram = np.array(img)\n",
    "    frames = slice_mel_spectrogram(mel_spectrogram)\n",
    "    for i, frame in enumerate(frames):\n",
    "        img_temporary = Image.fromarray(frame).convert('L')\n",
    "        img_temporary.save(\"data/Train_Sliced_Images_big/\"+\"_\"+track_id+'_'+str(i)+'_'+genre_variable+\".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b0c980-8f9b-49b6-be3b-2d61a1935bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = {\n",
    "\"Hip-Hop\": 0,\n",
    "\"International\": 1,\n",
    "\"Electronic\": 2,\n",
    "\"Folk\" : 3,\n",
    "\"Experimental\": 4,\n",
    "\"Rock\": 5,\n",
    "\"Pop\": 6,\n",
    "\"Instrumental\": 7\n",
    "}\n",
    "\n",
    "filenames = [os.path.join(\"data/Train_Sliced_Images_big\", f) for f in os.listdir(\"data/Train_Sliced_Images_big\")\n",
    "               if f.endswith(\".jpg\")]\n",
    "images_all = [None]*(len(filenames))\n",
    "labels_all = [None]*(len(filenames))\n",
    "for i,f in enumerate(filenames):\n",
    "    index = i\n",
    "    genre_variable = re.search(r\"_([^_]+)\\.jpg$\", f).group(1)\n",
    "    temp = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n",
    "    images_all[index] = temp\n",
    "    labels_all[index] = genre[genre_variable]\n",
    "\n",
    "images = np.array(images_all)\n",
    "labels = np.array(labels_all)\n",
    "labels = labels.reshape(labels.shape[0],1)\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, labels, test_size=0.05, shuffle=True)\n",
    "# Convert the labels into one-hot vectors.\n",
    "train_y = np_utils.to_categorical(train_y)\n",
    "test_y = np_utils.to_categorical(test_y, num_classes=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a6d8a4-63a7-4794-8133-186d29d9fcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4750, 256, 256), (4750, 8))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88c8a58d-2351-4900-b58a-91d4af424a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_classes = len(genre)\n",
    "genre_new = {value: key for key, value in genre.items()}\n",
    "\n",
    "if not os.path.exists('data/Training_Data'):\n",
    "    os.makedirs('data/Training_Data')\n",
    "np.save(\"data/Training_Data/train_x.npy\", train_x)\n",
    "np.save(\"data/Training_Data/train_y.npy\", train_y)\n",
    "np.save(\"data/Training_Data/test_x.npy\", test_x)\n",
    "np.save(\"data/Training_Data/test_y.npy\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4732e9a7-9394-4c4a-af16-07b6e326dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(\"data/Training_Data/train_x.npy\")\n",
    "train_y = np.load(\"data/Training_Data/train_y.npy\")\n",
    "test_x = np.load(\"data/Training_Data/test_x.npy\")\n",
    "test_y = np.load(\"data/Training_Data/test_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c4eab9b-6e0a-400c-9bd9-3f7619c1970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the dimensions of the image to have a channel dimension. (nx128x128) ==> (nx128x128x1)\n",
    "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], train_x.shape[2], 1)\n",
    "test_x = test_x.reshape(test_x.shape[0], test_x.shape[1], test_x.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1275db29-b3ca-4c2a-bd0f-7b5df5023772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the matrices.\n",
    "train_x = train_x / 255.\n",
    "test_x = test_x / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7691bf4e-5727-4e62-a4c3-313feabb9f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4750, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef4f465-4dc8-4b67-aa62-39050a893342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=[7,7], kernel_initializer = initializers.he_normal(seed=1), \n",
    "                 activation=\"relu\", input_shape=(128,128,1)))\n",
    "# Dim = (122x122x64)\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling2D(pool_size=[2,2], strides=2))\n",
    "# Dim = (61x61x64)\n",
    "model.add(Conv2D(filters=128, kernel_size=[7,7], strides=2, kernel_initializer = initializers.he_normal(seed=1), \n",
    "                 activation=\"relu\"))\n",
    "# Dim = (28x28x128)\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling2D(pool_size=[2,2], strides=2))\n",
    "# Dim = (14x14x128)\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "# Dim = (2048)\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(1024, activation=\"relu\", kernel_initializer=initializers.he_normal(seed=1)))\n",
    "# Dim = (1024)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation=\"relu\", kernel_initializer=initializers.he_normal(seed=1)))\n",
    "# Dim = (256)\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation=\"relu\", kernel_initializer=initializers.he_normal(seed=1)))\n",
    "# Dim = (64)\n",
    "model.add(Dense(32, activation=\"relu\", kernel_initializer=initializers.he_normal(seed=1)))\n",
    "# Dim = (32)\n",
    "model.add(Dense(train_y.shape[1], activation=\"softmax\", kernel_initializer=initializers.he_normal(seed=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc81d3c3-f94c-438c-99e6-a24b9f8dedec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 122, 424, 64)      3200      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 122, 424, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " average_pooling2d_2 (Averag  (None, 61, 212, 64)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 103, 128)      401536    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 28, 103, 128)     512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " average_pooling2d_3 (Averag  (None, 14, 51, 128)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 14, 51, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 91392)             0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 91392)            365568    \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 91392)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              93586432  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,639,208\n",
      "Trainable params: 94,455,784\n",
      "Non-trainable params: 183,424\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "plot_model(model, to_file=\"Saved_Model/Model_Architecture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4eb36c5e-4a4b-47ef-af24-f543faacef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer= Adam(learning_rate=0.0001), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c78dd0-fba0-4a59-bf21-3b7aafabd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hist = model.fit(train_x, train_y, epochs=10, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91795325-232a-41df-b348-3ceba6f6eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=1)\n",
    "print score\n",
    "model.save(\"Saved_Model/Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039096ea-459e-4109-81aa-1880b3366dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    \"\"\"Plots the accuracy and loss for a model over the course of all epochs\n",
    "    \n",
    "    Parameters:\n",
    "        hist (keras history object): The recorded history of model.fit() to be plotted\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8,7))\n",
    "    fig.tight_layout(pad=2)\n",
    "    \n",
    "    # Accuracy subplot\n",
    "    axs[0].plot(hist.history[\"acc\"], c='navy', label=\"Training Accuracy\")\n",
    "    axs[0].plot(hist.history[\"val_acc\"], c='orange', label=\"Validation Accuracy\")    \n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    \n",
    "    # Error subplot\n",
    "    axs[1].plot(hist.history[\"loss\"], c='navy', label=\"Training Loss\")\n",
    "    axs[1].plot(hist.history[\"val_loss\"], c='orange', label=\"Validation Loss\")    \n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    axs[1].set_xlabel(\"Epochs\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Loss\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5e528-d620-4549-9e46-7af86dcccb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(model_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d4285-1a05-467f-a2b3-da5bcd316140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
